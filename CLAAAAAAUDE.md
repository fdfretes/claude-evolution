Add to CLAUDE.md as permanent non-negotiable engineering standards for every session. These represent the highest standard of software engineering craftsmanship written from the perspective of a systems architect and standard-bearer. This document governs every session. Every file, commit, and decision passes through these standards. Do not ask for confirmation on any of these. Apply them silently. Only ask when intent is ambiguous. The commit log is the project's autobiography. git log --oneline should read like a changelog a new team member can understand without asking a single question. Every commit is a letter to a future debugger at 3am. Treat it accordingly. You are not writing code. You are building systems that outlive you. Clarity over cleverness: if a junior developer cannot read it in 6 months rewrite it because clever code is a liability and clear code is an asset. Simplicity is not the absence of complexity it is the mastery of it because the right abstraction makes the complex simple and the wrong abstraction makes the simple impossible. Every line must justify its existence so dead code speculative abstractions and just in case utilities must be deleted because the best code is the code you never wrote. Make it work then make it right then make it fast in that order and never optimize what is not correct and never ship what is not understood. Errors are not exceptions they are first-class citizens because a system that cannot fail gracefully is a system that cannot be trusted. The diff is the unit of communication so every change you make is a message to every future reader and you must write it like they are debugging at 3am with no context except your code and your commits. Write code you would be proud to show the person who taught you to code. Write commits you would be proud to show to the person who created Git. Build systems you would trust with your own data your own money your own time. The standard is not perfection the standard is intention with every line written with care every commit written with purpose and every system built to be understood maintained and trusted.

SECTION 1 GIT ENGINEERING STANDARDS. Follow Conventional Commits strictly. Format is type(scope): summary where type is one of feat fix docs style refactor perf test build ci chore revert. Subject line uses imperative mood lowercase no period and max 50 chars. Body when needed wraps at 72 chars and explains what and why not how. Footer references issues with Closes #number or Refs #number. Types map to semver: feat=MINOR, fix/perf=PATCH, all others=no bump. Append ! for BREAKING CHANGES (MAJOR) like feat!: remove legacy API. Body answers three questions: what was wrong, why this fix, what alternatives were rejected. Additional footer options: BREAKING CHANGE: description, Co-authored-by: Name email, Signed-off-by: Name email. If a commit needs a paragraph to explain it might need to be split. THE FIVE LAWS OF COMMITS. Law 1 Atomic and Bisect-Safe: every single commit MUST compile pass lint pass tests and run correctly IN ISOLATION. No commit may depend on a future commit to work. git bisect must be able to land on any commit and find a working system. If you need the word "and" to describe the commit it is two commits. Law 2 Separation of Concerns: NEVER mix in the same commit logic changes plus formatting or style changes, refactors plus new features, dependency updates plus code changes, bug fixes plus feature additions, file renames or moves plus content modifications. These are always separate commits in logical order. A rename commit contains ONLY the rename so git log --follow tracks correctly. Law 3 Reviewability-First Ordering: structure commits within a branch for the reviewer not for yourself. Order is 1 refactors and prep work that make the feature possible, 2 schema type interface changes, 3 core logic implementation, 4 tests, 5 documentation, 6 wiring and integration connecting everything. Each commit should be a self-contained chapter reviewable in sequence. Example ordering: refactor: extract payment module interface as prep work, build: add stripe SDK dependency as infrastructure, feat: implement stripe payment adapter as core logic, test: add stripe payment integration tests as verification, docs: add payment module API documentation, feat: wire stripe adapter into checkout flow as integration. Law 4 Zero Contamination: execute the Pre-Commit Protocol before EVERY commit with no exceptions. Never commit: secrets API keys tokens credentials .env files private keys, dead code commented-out code TODO hacks console.log or print debug statements, generated files (node_modules __pycache__ dist/ build/ .next/ coverage/ *.pyc .DS_Store), editor or IDE config unless project-shared (.vscode/settings.json ok, .idea/ not). Verify .gitignore is comprehensive BEFORE the first commit of any project. Law 5 History Is Immutable On Shared Branches: always explain before modifying any git history. Never on shared branches: git push --force, git push --force-with-lease, git rebase origin/main after pushing, git reset --hard HEAD~n. Always for undoing shared commits: git revert sha -m "Revert: reason why original change is being undone". Allowed on your unpushed feature branches: git rebase -i origin/main to clean up before PR, git commit --amend to fix last commit, git rebase -i --autosquash to fold fixups. PRE-COMMIT PROTOCOL executed every time with no exceptions. Step 1 verify scope by running git diff --staged --stat and asking are ONLY intended files staged. Step 2 line-by-line review by running git diff --staged and reading every line understanding every change. Step 3 security scan by running git diff --staged piped to grep -iE for secret key token password api_key credential private_key followed by equals or colon pattern and if ANY match stop unstage and remediate. Step 4 contamination check by running git diff --staged piped to grep -nE for console.log console.debug print( debugger TODO FIXME HACK XXX and evaluating if matches are intentional or leftover. Step 5 verify atomicity by asking can I describe this commit WITHOUT the word "and" and would git bisect landing here find a working system. Step 6 compose message following Conventional Commits format. BRANCHING AND HISTORY MANAGEMENT. Branch naming: feat/short-desc, fix/issue-N-desc, chore/desc, hotfix/critical-desc, docs/api-endpoint-reference, refactor/extract-user-module, test/payment-edge-cases. Always branch from latest main unless instructed otherwise. Linear history preferred so rebase feature branches onto target before merge. Before pushing: git fetch origin && git rebase origin/main and resolve conflicts properly never using --skip without understanding. Push: always git push -u origin branch-name. Stacked changes: for large features break into sequential PRs that each stand alone like feat/payments-base for interfaces and types as PR #1, feat/payments-stripe for adapter depending on #1 as PR #2, feat/payments-checkout for integration depending on #2 as PR #3. ADVANCED GIT WORKFLOWS. Patch-mode staging: use git add -p to stage individual hunks when a file contains changes for multiple logical concerns. Fixup workflow: git commit --fixup=sha during development then git rebase -i --autosquash origin/main to fold fixups into their target commits. Cherry-pick provenance: note the original SHA in the commit body as "Cherry-picked from sha" with context about why. Changelog generation: when asked generate from conventional commit history grouped by type. THE NEWSPAPER TEST. Before pushing, run git log --oneline origin/main..HEAD and answer honestly: could a new team member understand what this branch accomplishes from this alone? Does each line represent exactly one logical change? Are they in an order that builds understanding incrementally? Would you present this history in a code review with pride? If any answer is no then git rebase -i origin/main and fix it before pushing.

SECTION 2 CODE ARCHITECTURE. Dependency Direction is strict: handlers/ depends on services/ which depends on core/. Both handlers/ and services/ can depend on adapters/. Adapters/ interface with external systems like databases APIs file systems and hardware. core/ contains pure business logic with zero imports from adapters handlers or external packages. Only stdlib and internal types. Every function here must be unit-testable with no mocks. services/ handles orchestration composing core logic with adapters and owns transaction boundaries. adapters/ handles all I/O including database queries HTTP clients file system message queues and hardware interfaces. It implements interfaces defined by core/services. DEPENDENCY DELIVERY: services receive adapters via constructor or factory parameters never via direct import of adapter instances. This makes dependencies explicit testable and swappable. Right is createUserService({ userRepo, emailAdapter }) where the caller wires dependencies. Wrong is import { userRepo } from '../adapters/db' at module level inside the service which creates hidden coupling and prevents testing without module mocking hacks. handlers/ are entry points only: HTTP routes CLI commands event consumers WebSocket handlers. They are thin: parse input then call service then format output. Max 15-20 lines per handler. config/ is loaded once at startup validated with schema injected explicitly. Never import config inside business logic. Violation example: core/pricing.ts importing db from adapters/database is NEVER allowed because core must have zero external dependencies. A handler containing business logic like hashing passwords or inserting into database directly is wrong because that belongs in services/ and adapters/ respectively. The correct pattern is: handler parses and validates input using parseCreateUserInput(req.body), delegates to service using await userService.create(input), and formats and returns output using res.status(201).json(formatUser(user)). FILE ORGANIZATION. src/ contains core/ with models/ for types interfaces schemas enums with no logic, services/ for business operations pure where possible, errors/ for typed error classes where every domain error is a class, utils/ for pure utility functions under 50 lines each with no I/O. adapters/ contains db/ for query builders repositories migrations, http/ for external API clients, storage/ for file blob S3 operations, hardware/ for serial GPIO sensor interfaces. handlers/ contains api/ for HTTP REST GraphQL route handlers, cli/ for CLI command handlers, events/ for message queue event consumers, ws/ for WebSocket handlers. config/ contains index.ts to load validate freeze export config object and schema.ts for config validation schema. middleware/ for auth logging rate limiting error handling. tests/ contains unit/ mirroring src/core/ structure, integration/ mirroring src/adapters/ structure, e2e/ for critical user workflows, fixtures/ for shared test data factories. scripts/ for build deploy seed migrate scripts. docs/ for architecture decisions API docs runbooks. Rules: one exported concept per file where file name equals exported name in lowercase with dashes like user-service.ts exports UserService. Max 300 lines per file and over 300 means split by sub-responsibility. Index files re-export only with zero logic in index files. Co-locate tests: user-service.ts has user-service.test.ts or use parallel tests/unit/ tree. Circular imports are architecture bugs so if A imports B and B imports A extract shared code to C. DESIGN FOR DELETION. Before adding any module verify: can I delete this module by removing the directory and removing its import from 1-2 wiring files and being done? If removing it requires modifying more than 3 files the coupling is wrong and you must refactor the boundaries.

SECTION 3 CODE STANDARDS. NAMING CONVENTIONS. Variables describe what it IS: remaining_retries = 3 not r = 3. Functions are verb plus noun: validate_user_input() not process() or handle_data(). Booleans read as yes/no questions: is_active has_permission can_retry not active or flag. Constants describe MEANING with units: MAX_RETRY_ATTEMPTS = 3 and TIMEOUT_MS = 5000 not THREE = 3. Collections are plural nouns: users not user_list or data. Classes are specific PascalCase nouns: PaymentProcessor not ProcessManager or Helper. Never name anything Helper Manager Handler without specificity. No Hungarian notation: CreateUserInput not IUser. Files use kebab-case: user-service.ts. Never vague names like utils.ts or helpers.ts. FUNCTIONS HARD CONSTRAINTS. Max 3 parameters and beyond 3 use an options object with named fields and types. This applies to signatures you define not callback signatures imposed by frameworks or runtime APIs. Example options object: interface CreateUserOpts with name string email string role UserRole orgId string sendWelcomeEmail optional boolean defaulting to true. No boolean parameters because createUser("alice", true) is unreadable at the call site so instead use createUser("alice", { sendWelcomeEmail: true }) or create separate functions like createAdminUser("alice"). Return early and never nest beyond 3 levels: wrong is nested if statements checking user then user.isActive then user.hasPermission with actual logic buried 3 levels deep. Right is guard clauses at the top: if not user return null, if not user.isActive throw InactiveUserError(user.id), if not user.hasPermission throw PermissionError(user.id, 'process'), then actual logic at top level. Pure functions in core/ where same input always gives same output with no I/O like calculateDiscount(price, tier) returns price * DISCOUNT_RATES[tier]. Side effects live in adapters/ like async saveOrder(order) that awaits db.orders.insert(order). Max 30 lines per function and if longer extract sub-functions with descriptive names that serve as documentation. ERROR HANDLING. Create a base AppError class extending Error with constructor taking message string, code string that is machine-readable like USER_NOT_FOUND, statusCode number for HTTP status if applicable, and optional context Record of string to unknown for debug data. Set this.name to this.constructor.name in the constructor. Create domain errors extending AppError: NotFoundError takes entity string and id string and constructs with message "${entity} not found: ${id}" code NOT_FOUND status 404 context { entity, id }. ValidationError takes field string and reason string and constructs with message "Invalid ${field}: ${reason}" code VALIDATION_ERROR status 400 context { field, reason }. Rules: NEVER swallow errors because catch (e) {} is criminal and catch (e) { console.log(e) } is negligent since the error is lost to logs and the caller does not know. ALWAYS add context when rethrowing by throwing new AppError with descriptive message including the identifier like orderId, appropriate code and status, and context including the original error message. Error messages answer THREE questions: what happened like "Failed to connect to database", what was expected like "Expected connection within 5000ms", and what to do like "Check DATABASE_URL env var and database status". Boundary error handling pattern: handlers catch and format, services throw, core returns typed errors. Handler is the catch boundary: try to parse input and call service and return json, catch and pass to next for global error handler. Global error handler is one place one format: check instanceof AppError for status and code defaulting to 500 and INTERNAL_ERROR, log server errors for status 500+, return structured json with error object containing code and message. RESOURCE CLEANUP. Every acquired resource must be released in a finally block or equivalent language construct like Python with or using/await using. Pattern: acquire then try to use then finally release regardless of outcome. Database transactions: begin in try, commit at end of try, rollback in catch, release connection in finally. Same principle for file handles streams event listeners and any object with close or dispose. Never rely on garbage collection for external resources. GRACEFUL SHUTDOWN. Every long-running process handles SIGTERM: 1 stop accepting new work by closing the server listener, 2 wait for in-flight requests to complete with a timeout of 30 seconds, 3 close connection pools and flush buffers, 4 exit with code 0. This is required for zero-downtime deployments per Sections 10 and 21. TYPE SAFETY. No any â€” use unknown and narrow with type guards. If any is unavoidable (FFI, third-party type deficiency), accompany with a comment explaining why unknown is insufficient. No type assertions using as unless accompanied by a runtime check or a comment explaining why it is safe. Validate at boundaries using Zod Joi or equivalent at every entry point including API CLI config and file read and after validation the type is trusted. Prefer discriminated unions over optional fields when states are mutually exclusive: wrong is a single Order interface with status string union and optional trackingNumber and optional deliveredAt that allows impossible states like pending with a tracking number. Right is a union type where Order is either { status: 'pending' } or { status: 'shipped', trackingNumber: string } or { status: 'delivered', trackingNumber: string, deliveredAt: Date } making impossible states unrepresentable. COMMENTS. Wrong is describing what code does because the code already says that like "Increment counter by 1" above counter += 1. Wrong is dead commented-out code like a commented const oldValue = calculateLegacy(x) because that is what version control is for. Right is explaining WHY like "Offset by 1 because the sensor reports 0-indexed but the API expects 1-indexed" above counter += 1. Right is documenting non-obvious business rules like "Orders over $500 require manager approval per compliance policy CP-2024-03" above the approval check. Right is warning about gotchas like "IMPORTANT: This API returns dates in UTC but without timezone suffix. Always append 'Z' before parsing or you'll get local-time bugs in production."

SECTION 4 DEPENDENCIES AND CONFIGURATION. DEPENDENCY DECISION FRAMEWORK. Before npm install or pip install anything ask: 1 can I write this in under 50 lines? If yes write it and own it. 2 Is it a trivial utility like leftpad or is-odd? Absolutely not and write it. 3 Is it complex domain like crypto parsing or compression? Use a dependency. 4 Check the package: last published over 1 year ago with open security issues means find alternative. Transitive deps over 20 for a utility means too heavy find alternative. Bundle size blows your budget means find alternative. License MIT Apache BSD is OK, GPL means careful, unlicensed means never. 5 Can I wrap it in an adapter so I can swap it later? Always yes and do this. Wrapper pattern: wrong is importing dayjs directly scattered across 47 files in the codebase making it impossible to swap. Right is wrapping in adapters/date.ts that imports dayjs internally and exports functions like formatDate(date, format) and addDays(date, days). Rest of codebase imports from adapters/date never from dayjs directly so if you need to swap the library you change one file. CONFIGURATION PATTERN. Define config schema with Zod or equivalent including port as number with default 3000, databaseUrl as required string url, redisUrl as optional string url, jwtSecret as string with min 32 characters, environment as enum of development staging production, logLevel as enum of debug info warn error with default info. Load config once by parsing environment variables through the schema using safeParse. If validation fails console.error the flattened errors and process.exit(1) to FAIL AT STARTUP not at first request 3 hours after deployment. Export as Object.freeze(parsed.data) so config is immutable. Provide sensible defaults for development and require explicit configuration for production. Never read process.env in random places inside business logic: wrong is reading process.env.SENDGRID_KEY inside a sendEmail function where if it is undefined you find out at runtime when a customer tries to get an email. Right is injecting via config or constructor like createEmailService(apiKey) where apiKey was validated at startup and is guaranteed present.

SECTION 5 TESTING. TEST STRUCTURE. Use describe blocks for the service and nested describe for the method and it blocks with descriptive names. Follow Arrange-Act-Assert pattern: ARRANGE sets up inputs and dependencies like building a CreateUserInput and creating a mock repo, ACT executes exactly one thing like calling userService.create(input, { userRepo: mockRepo }), ASSERT verifies outcomes like checking result.email equals expected and result does not have password property and mockRepo.save was called with expected data. Test names describe the scenario and expected outcome like "should create user with hashed password and return without password field" and "should throw ValidationError when email already exists" and "should throw ValidationError when email format is invalid". TEST DATA FACTORIES. Never use raw literals in tests. Create factory functions like buildUser(overrides: Partial<User> = {}) that returns a complete User object with sensible defaults using randomUUID for id, randomized email like test-${randomUUID()}@example.com, name defaulting to Test User, role defaulting to member, createdAt defaulting to new Date, and spreading overrides. Similarly buildCreateUserInput with defaults for email name and password. Usage only specifies what matters for each specific test like buildUser({ role: 'admin' }) for testing admin behavior or buildUser({ createdAt: new Date('2025-01-01') }) for testing date-based logic. TESTING RULES. Every bug fix ships with a regression test that reproduces the bug and this is non-negotiable. Test names follow should_expected_when_condition format which reads as specification. One logical assertion per test so a failing test immediately tells you what broke. No test interdependence where each test runs in isolation with no shared mutable state between tests. Test behavior not implementation: never test private methods or internal state. If you refactor internals and tests break the tests were wrong. Test the contract not the implementation. Mock at boundaries only: mock adapters like DB and HTTP but never mock core logic or services. Flaky test equals P1 bug to fix or delete immediately because flaky tests erode trust in the entire suite. Integration tests use real dependencies with Docker containers for DB Redis etc because mocking a database in integration tests defeats the purpose. Do not test frameworks or libraries because you did not write Express or Django so do not test that they work. COVERAGE STRATEGY. Core logic targets 90%+ coverage using unit tests and pure function testing because this is where bugs cost the most. Adapters target 70%+ using integration tests with real dependencies to verify queries API calls and serialization. Handlers cover critical paths using e2e tests for the 5-10 workflows that matter most. Do not chase 100% because of diminishing returns. Test decision points and edge cases not getters and trivial constructors.

SECTION 6 SECURITY. INPUT VALIDATION AT EVERY BOUNDARY. Every handler validates input with a schema before processing using .parse() which throws on invalid input. After that point input is typed and trusted throughout the service layer. Schema definitions are strict: email uses z.string().email().max(254), name uses z.string().min(1).max(100).trim(), password uses z.string().min(12).max(128). Use .strict() on schemas to reject unknown fields preventing mass assignment attacks. INJECTION PREVENTION. SQL is ALWAYS parameterized: wrong is string interpolation like SELECT * FROM users WHERE email = '${email}' which allows SQL injection. Right is parameterized query like SELECT * FROM users WHERE email = $1 with the value passed as an array parameter [email]. Shell commands NEVER interpolate user input: wrong is exec(`convert ${userFilename} output.png`) which allows remote code execution. Right is execFile('convert', [userFilename, 'output.png']) with arguments as an array. HTML ALWAYS escapes: wrong is element.innerHTML = userInput which allows XSS. Right is element.textContent = userInput. RESPONSE SERIALIZATION SAFETY. Never serialize domain objects or database rows directly to API responses. Define explicit response types with an allowlist of fields for each endpoint. Use a mapping function like toUserResponse(user) that picks only the fields the client should see. Denylist approaches that omit sensitive fields fail silently when new columns are added to the database. The allowlist is the contract: if a field is not explicitly included it does not leave the system. SECRETS MANAGEMENT. Gitignore minimum that must be verified BEFORE first commit in ANY project: .env, .env.*, !.env.example (committed), *.pem, *.key, *secret*, *credential*, node_modules/, __pycache__/, dist/, build/, .next/, coverage/, .DS_Store, *.pyc, .idea/, *.sqlite, *.db. Commit .env.example documenting all required environment variables WITHOUT actual values so new developers know what to configure. Never log secrets: wrong is logging config.databaseUrl which contains the connection string with credentials. Right is logging only config.dbHost and config.dbName without any credentials. AUTH PATTERN. Authenticate middleware: verify token once and attach verified identity to request context. Extract bearer token from authorization header, return 401 UNAUTHORIZED if missing. Verify JWT with config.jwtSecret, attach payload to req.user, call next(). Catch verification errors and return 401 INVALID_TOKEN. Authorization middleware: check role AFTER authentication BEFORE action. Create requireRole factory that takes allowed roles and returns middleware that checks if req.user.role is included in allowed roles, returning 403 FORBIDDEN if not. Usage is layered: app.delete('/api/users/:id', authenticate, requireRole('admin'), deleteUserHandler) applying auth then authorization then handler.

SECTION 7 PERFORMANCE. DATABASE IS SOURCE OF MOST LATENCY. N+1 detection: if you see a loop where each iteration makes a separate database query that is N+1 and never acceptable. Example: fetching all users then looping to fetch orders for each user means 100 users equals 101 queries. Fix with JOIN: SELECT u.id, u.name, u.email, json_agg(o.*) as orders FROM users u LEFT JOIN orders o ON o.user_id = u.id GROUP BY u.id (enumerate columns per the SELECT * rule below). Or batch load: fetch all users first, extract all user IDs, fetch all orders WHERE user_id = ANY($1) with the ID array, then associate in application memory. Pagination is mandatory on every list endpoint: clamp limit between 1 and 100 with default 20, calculate offset from page number, use LIMIT and OFFSET in queries, return total count alongside results. OFFSET is O(n) in skipped rows so for tables exceeding 10K rows or deep page access use keyset/cursor pagination per Section 13 instead. Never SELECT * in production queries and select only the columns you need. Index strategy: add index for any column in WHERE JOIN ON or ORDER BY that is queried frequently. Do not add index for rarely queried columns small tables under 1000 rows or columns with low cardinality. Composite indexes: leftmost column must match query so index (a, b, c) covers WHERE a= and WHERE a= AND b= but NOT WHERE b= or WHERE c= alone. ASYNC PATTERNS. Wrong is sequential awaits when operations are independent: const users = await getUsers() then const orders = await getOrders() then const products = await getProducts() where total time is the sum of all three. Right is parallel when independent: const [users, orders, products] = await Promise.all([getUsers(), getOrders(), getProducts()]) where total time is the max of the three. Handle partial failures with Promise.allSettled: get all results, filter by status fulfilled to get successes mapping to value, filter by status rejected to get failures mapping to reason. PERFORMANCE BUDGETS. API response at p95 under 200ms for reads and under 500ms for writes, profile and optimize if exceeded. Database query under 50ms, add index or optimize query if exceeded. Startup time under 5 seconds, lazy-load non-critical modules if exceeded. Memory per request under 50MB, stream large datasets if exceeded. Frontend bundle under 200KB gzipped, code-split and tree-shake if exceeded.

SECTION 8 LOGGING AND OBSERVABILITY. STRUCTURED LOGGING. Wrong is console.log with string concatenation like 'User created: ' + user.email which is unstructured and unsearchable. Right is logger.info with event name 'user_created' and structured data object containing userId, email, source, duration_ms. For errors use logger.error with event name 'order_processing_failed' and structured data containing orderId, error message, stack trace, and retryable boolean. LOG LEVELS USED CORRECTLY. error is for something failed that should not have and needs attention like DB connection lost payment failed or unhandled exception. warn is for something unexpected but handled and should be reviewed if recurring like rate limit approached deprecated API called or fallback used. info is for significant business events serving as the audit trail like user created order placed or deployment started. debug is for developer diagnostics off in production by default like query parameters cache hit or miss or intermediate calculations. REQUEST ID PROPAGATION. Middleware generates a request ID (UUID) if no X-Request-ID header is present and uses the incoming one if present. Attach to request context so the logger includes it automatically without manual passing. Forward the header on all outbound HTTP calls to downstream services. For async messages include the originating request ID in the message envelope. Rules: never log PII like emails names or addresses without masking like user_***@example.com. Never log credentials tokens or secrets period. Every log entry includes timestamp level service name and request ID for tracing. Request IDs propagate across service boundaries for distributed tracing.

SECTION 9 DEVELOPMENT WORKFLOW. BEFORE WRITING CODE. 1 UNDERSTAND the requirement by restating it in your own words and if ambiguous ask. 2 SEARCH for existing solutions by searching the codebase with grep -r and file search because you must not reinvent what exists. 3 DESIGN the interface before the implementation by defining what the caller sees what the function signature looks like what the API contract looks like and coding from the outside in. 4 Identify BOUNDARIES for what is core logic what is an adapter what is a handler and whether a new module is needed. 5 PLAN COMMITS for how this will break into atomic reviewable commits. WHILE WRITING CODE. 1 Write the test first when the behavior is well-defined because it clarifies thinking and prevents scope creep. 2 Run the code every 20-30 lines not 200 lines then test. 3 Commit often on your feature branch at every logical checkpoint. Clean up before merging. 4 If you are stuck for more than 15 minutes step back re-read the requirement simplify and ask. 5 Every new file verify it belongs in the right directory per the architecture rules. BEFORE SUBMITTING. Run git diff origin/main..HEAD to self-review the entire diff reading it as a stranger would asking is it obvious and is it minimal. Run npm test or python -m pytest or go test ./... for the full test suite not just the file you changed. Run npm run lint or ruff check . or golangci-lint run for zero warnings. Run npx tsc --noEmit for zero type errors if TypeScript. Run npm audit or pip audit for security vulnerability check. Run git diff --staged piped to grep -iE for secret key token password to scan for secrets. Run git rebase -i --autosquash origin/main to squash fixup commits and reorder for clean history. Run git log --oneline origin/main..HEAD for the Newspaper Test. Update documentation if behavior changed, update .env.example if new config added, document why in commit body if new dependency added. Then git push -u origin feat/my-feature.

SECTION 10 DEPLOYMENT AND INFRASTRUCTURE. Everything in version control including infrastructure config scripts Dockerfiles. Deployments are automated with no manual SSH and copy, repeatable with same input giving same output, reversible with rollback in under 5 minutes, and observable so you know immediately if it broke. Environments go development to staging to production where staging mirrors production exactly with same infrastructure scaled down. Anything that passes staging deploys to production automatically or with one approval. Feature flags over long-lived feature branches for features in production. FEATURE FLAG LIFECYCLE. Every flag has a type: release (temporary, remove within 30 days of full rollout), ops (kill switches, may be permanent), experiment (temporary, remove after decision), or permission (long-lived, managed like a feature). Every flag has an owner. Release and experiment flags that outlive their window are stale flags treated as dead code per the preamble: delete them. Never reuse a flag name. DOCKER STANDARDS WHEN USED. Multi-stage build: FROM node:20-slim AS builder for WORKDIR /app, COPY package*.json, RUN npm ci --production=false to install all deps for build, COPY source, RUN npm run build. Then FROM node:20-slim AS runtime for WORKDIR /app, COPY --from=builder only dist and node_modules and package files. Create non-root system user and group with addgroup --system app and adduser --system --ingroup app app, switch with USER app. EXPOSE port. HEALTHCHECK --interval=30s --timeout=5s CMD curl -f http://localhost:3000/health. CMD to run the application. Dockerignore mirrors gitignore plus more: node_modules .git .env .env.* dist coverage *.md .vscode .idea tests. HEALTH CHECKS. Every service exposes GET /health that runs checks on database and redis connectivity, reports uptime via process.uptime() and memory via process.memoryUsage().heapUsed and version from config. Determines healthy boolean from check results. Returns status 200 with { status: 'ok', checks } when healthy or status 503 with { status: 'degraded', checks } when not.

SECTION 11 COMMUNICATION PROTOCOL. THE BRIGHT LINE: conforming to standards in new code is silent; restructuring existing code or architecture is ask-first. APPLY SILENTLY EVERY TIME: all standards in this document when writing new code or extending existing patterns, Conventional Commits format, pre-commit verification protocol, typed errors with context, input validation at boundaries, structured logging, choose the simplest correct solution, handle errors properly, write self-documenting code, adding new files that follow existing directory structure and conventions like a new handler in handlers/ or adapter in adapters/. ASK BEFORE DOING: adding any external dependency, restructuring existing code or moving responsibilities between layers, creating new directories or architectural patterns not already established, any operation that modifies shared or published git history, breaking changes to public APIs or database schemas, deleting data or files that are not obviously temporary or generated, a change touching more than 5 unrelated files or more than 300 lines. NEVER DO: commit or output secrets credentials API keys .env contents, use any types or suppress linter warnings without justification, leave console.log debugger or uncommented TODO without ticket reference, force push to any shared branch, add dependencies for trivial operations under 50 lines to implement, catch errors without handling or rethrowing with context, write comments that describe what code does instead of why, generate verbose or padded code to seem thorough. WHEN UNCERTAIN: state the tradeoff explicitly with max 2 options with a clear recommendation and rationale for why. If a requirement conflicts with these standards flag it and never silently compromise. If you do not know something say so and do not guess.

SECTION 12 DEBUGGING AND FIX COVENANT. THE PRIME DIRECTIVE: understand the bug BEFORE touching the code. A fix without understanding is a second bug hiding behind the first. If you cannot explain WHY it broke you have not found the root cause. PHASE 1 REPRODUCE BEFORE EVERYTHING. No reproduction means no fix attempt period. Get the exact failure: what was the input, what was the expected output, what was the actual output, what environment including OS runtime version DB state and config. Write the reproduction FIRST as a failing test that MUST fail before you touch any source code. Example test for bug #247: test that processOrder crashes with TypeError when auth token contains valid signature but missing user claim and this test currently fails with fix incoming next commit. If you cannot reproduce it: check logs with request ID or trace ID, check if it is environment-specific by diffing staging vs production config, check if it is timing or race condition by adding logging not guessing, check if data-dependent by examining what is in the DB for that user or record. NEVER apply a speculative fix to a bug you cannot reproduce. PHASE 2 ISOLATE THE ROOT CAUSE. Use binary search of the problem space not linear code reading hoping to spot it. Technique 1 WHERE in the stack does it break: add a log or breakpoint at each boundary between handler service core adapter and external system. Find the EXACT layer. Technique 2 WHAT is the state at the failure point: log the actual values and do not assume. The bug is always in the gap between what you THINK the state is and what it ACTUALLY is. Technique 3 WHEN was it introduced: use git bisect to binary-search commits between a known-good and known-bad state. Automate with git bisect run <test-command> to find the exact introducing commit automatically. Technique 4 WHY did it break using Five Whys technique: ask why five times drilling from symptom to root cause. Example: why did the order fail? Because userId was null. Why was userId null? Because the auth middleware did not reject the request. Why did auth not reject it? Because the token was valid but had no user claim. Why did the token have no user claim? Because it was issued by legacy system. Why is legacy system issuing claimless tokens? Because migration is incomplete. ROOT CAUSE: missing validation at token verification boundary. Not "userId was null" because that is a symptom not a cause. PHASE 3 UNDERSTAND THE BLAST RADIUS. Before writing a single line of fix answer these questions. 1 What else does this code path affect? Use grep -rn "functionName" src/ to find who calls this function. Use grep -rn "import.*module" src/ to find who depends on this module. 2 What assumptions does downstream code make? If the function currently returns null on failure and you change it to throw then every caller that does not try/catch will now crash. 3 Does this bug exist in other similar code paths? If the bug is missing null check on userId search for EVERY place userId is accessed without validation and fix them ALL not just this one. 4 Can the fix break existing tests? Run the FULL suite before and after not just the file you changed. 5 Is this a symptom of a deeper architectural problem? If yes document it. Fix the immediate bug now and file a ticket for the architectural fix. Do not scope-creep a hotfix into a refactor. PHASE 4 THE FIX. Fix structure is three commits minimum for non-trivial bugs. Commit 1 the failing regression test like test(orders): add null userId regression test with body noting the test currently fails and fix follows. Commit 2 the minimal fix like fix(orders): validate userId before processing with body explaining root cause, what the fix does, what was wrong, and Closes the issue. Commit 3 defensive fixes for similar patterns found in Phase 3 blast radius search like fix(services): add userId checks across services. All commit messages follow Section 1 Conventional Commits format with body answering what was wrong, why this fix, what alternatives rejected. FIX QUALITY RULES. Rule 1 minimal diff: fix the bug and nothing else. Wrong is fixing the bug AND refactoring AND renaming AND adding a feature. Right is bug fix only with refactor as a separate commit or PR. Rule 2 fix at the RIGHT layer not where the symptom appears: wrong is adding null check at every callsite like if (user && user.id) repeated in 15 places which treats symptoms. Right is validating once at the boundary with a validateAuthContext function that checks userId is present throws ValidationError if not and returns a ValidatedAuth type that is trusted from that point forward. This treats the root cause. Rule 3 make the illegal state unrepresentable: wrong is an optional userId that should always be there with an interface AuthContext where userId is optional string and sometimes null hoping for the best. Right is a ValidatedAuth type with userId as required string guaranteed present after validation at the boundary. Rule 4 never fix with a try/catch wrapper unless the error is truly recoverable: wrong is catching processOrder and logging "order failed continuing" which is criminal because it hides what order failed and why and what happens now. Right is letting it crash loud where the service layer throws and the handler catches and formats the error response and logs with full context. Rule 5 never fix with a comment: wrong is adding TODO this sometimes returns null need to investigate while the code still crashes on the next line. Right is fixing it now with a proper null check and error throw or throwing with context if you cannot fix the underlying issue yet. PHASE 5 VERIFY THE FIX. The failing reproduction test now passes. Run the full test suite not just your file. Lint clean with zero warnings. Type check with zero errors if applicable. Manual verification against original reproduction steps following them exactly as reported. Edge case testing around the boundaries of the fix: what about empty string userId which is not null but still invalid? What about userId that exists in token but user was deleted from DB? What about concurrent requests during this state? PHASE 6 POST-FIX DOCUMENTATION. In the regression test document the FULL context with a detailed comment block. Include the bug number, a description of the root cause with the full chain of causation like "Legacy token issuer v1 auth service issues valid JWTs without the sub claim. The v2 auth middleware validates signature but does not verify claim presence. processOrder accessed userId without null check because it assumed middleware guaranteed it." Document what the fix was and what the architectural fix is tracked in as a separate issue. Document exact reproduction steps like "Send POST /api/orders with v1-issued token valid signature no sub claim." DEBUGGING DECISION TREE. Follow Phases 1-6 in order. Use these branching questions to choose technique: Can you reproduce it? If no follow Phase 1 cannot-reproduce checklist. Is it a regression? If yes use git bisect (Phase 2 Technique 3). Is it environment-specific? Diff configs dependencies and data between environments. Is it timing-dependent? It is a race condition: add mutex lock or queue and test under concurrency. Otherwise it is deterministic logic: isolate with Phase 2 techniques and fix. ANTI-PATTERNS THAT ARE NEVER ACCEPTABLE. Shotgun debugging: violates Prime Directive by changing random things without understanding. Speculative fixing: violates Phase 1 by guessing instead of reproducing. Fixing the symptom instead of the cause: violates Rule 2 by adding checks at callsites instead of validating at boundaries. Scope creep during bugfix: violates Rule 1 by mixing fixes with refactors or features. Closing the ticket without a regression test: violates Phase 4 commit structure. Printf debugging committed to production: use the Temporary Debug Logging Protocol below, never push console.log to prod. Blaming the framework library or OS without evidence: it is your bug 99.9% of the time so prove it is not your code before blaming external systems by writing a minimal reproduction outside your codebase. Fixing in production manually: SSHing into prod editing a file and restarting with no version control no review no audit trail is NEVER acceptable and all fixes go through the pipeline. EMERGENCY HOTFIX PROTOCOL. When production is DOWN and you need to fix NOW you still follow the process just faster. Step 1 git checkout -b hotfix/critical-description origin/main. Step 2 write the minimal reproduction even if it is just a manual curl command documented in the commit message. Step 3 apply the SMALLEST possible fix. Disable the broken feature if needed because a feature flag turning off broken code is better than a rushed fix that breaks something else. Do not attempt complex fixes under pressure. Step 4 test what you can with at minimum existing tests passing by running npm test. Step 5 deploy through the pipeline fast-tracked not skipped: push the branch open a PR get fast review merge and auto-deploy. Step 6 AFTER production is stable: write the proper regression test, investigate root cause fully not just the symptom you patched, write a post-mortem covering what happened why it happened and what changes prevent recurrence, file follow-up tickets for proper fix if the hotfix was a band-aid. Hotfix commit message example: fix(auth): disable legacy token acceptance with body explaining PRODUCTION INCIDENT with unauthorized access via legacy v1 tokens that bypass claim validation. Disabling legacy token acceptance as immediate mitigation. Proper fix to add claim validation to v2 middleware tracked in follow-up issue. Post-mortem scheduled for tomorrow. Closes the incident number. TEMPORARY DEBUG LOGGING PROTOCOL. When adding debug logging to investigate a bug: 1 use DEBUG level never info warn or error for temporary debug logging. 2 Prefix all debug additions with the bug number so they are easy to find and remove like DEBUG-247:order_entry and DEBUG-247:after_validation and DEBUG-247:pre_db_write. 3 NEVER log full tokens passwords PII or request bodies with sensitive data even in debug logging. 4 Remove ALL debug logging before merging by running git diff --staged piped to grep for the debug prefix like DEBUG-247 and if any matches remain you forgot to clean up so remove them before committing. 5 If the logging turns out to be USEFUL beyond debugging convert it to proper structured logging as a separate commit before merging with a message like chore(logging): add order processing observability. Temporary debug logging and permanent observability improvements are separate commits.

SECTION 13 API DESIGN STANDARDS. Every API is a contract. Once published it is a promise. Break the promise and you break every consumer. REST conventions: use nouns not verbs in URLs. GET /api/users not GET /api/getUsers. Use plural nouns: /api/users not /api/user. Nest resources to show relationships: /api/users/:id/orders not /api/getUserOrders. HTTP methods map to operations: GET for read with no side effects ever, POST for create returning 201 with Location header, PUT for full replace of a resource, PATCH for partial update, DELETE for removal returning 204 with no body. Status codes are not suggestions. Use standard codes correctly. Non-obvious distinctions: 401 for missing or invalid authentication vs 403 for valid auth but insufficient permissions. 409 for duplicate or state conflict. 422 for valid syntax but invalid semantics. 429 with Retry-After header. Response envelope: always return { data: ... } for success and { error: { code: string, message: string, details?: ... } } for errors. Never return raw arrays at the top level because you cannot extend the response later without breaking clients. Pagination contract: every list endpoint returns { data: items[], meta: { page, limit, total, totalPages } }. Support both offset pagination with page and limit parameters and cursor pagination with cursor and limit for large datasets. Use cursor pagination when data changes frequently or datasets exceed 10000 records. API versioning: use URL versioning /api/v1/users as the default because it is explicit visible and easy to route. Never remove fields from responses without a version bump. Adding fields is non-breaking. Removing or renaming fields is breaking. Changing field types is breaking. Always support the previous version for at least 6 months after deprecation notice. DEPRECATION LIFECYCLE. Deprecated endpoints return Deprecation header (RFC 9745) with timestamp and Sunset header (RFC 8594) with removal date on every response. Mark deprecated: true in API specification with migration guidance to the replacement. Log calls to deprecated endpoints at warn level per Section 8. Monitor traffic and do not remove while active consumers remain. After the sunset date return 410 Gone with Link header to the replacement not 404. Removal requires zero traffic for 30 days after the sunset date. Rate limiting: every API has rate limits. Return X-RateLimit-Limit X-RateLimit-Remaining and X-RateLimit-Reset headers on every response. Return 429 with Retry-After when exceeded. Use sliding window algorithm not fixed window. Different limits for authenticated vs unauthenticated and different tiers if applicable. Idempotency: all POST and PATCH endpoints should support an Idempotency-Key header. Store the key and response for 24 hours. If the same key is sent again return the stored response without re-executing. This prevents duplicate side effects from network retries.

SECTION 14 DATABASE AND DATA MODELING. Schema design principles: normalize first then denormalize strategically with measurement. Third normal form is the default. Denormalize only when you have measured a specific query performance problem and the denormalization solves it. Every denormalization is technical debt that needs a comment explaining why it exists and what query it optimizes. Primary keys: use UUIDs v4 or v7 not auto-increment integers for public-facing IDs. Auto-increment leaks information about volume and creation order. UUIDv7 is preferred when available because it is time-ordered and plays well with B-tree indexes. Keep auto-increment as internal database primary key if needed for joins and add a separate uuid column for external reference. Soft deletes vs hard deletes: default to soft deletes with a deleted_at timestamp column for any data that has business significance or audit requirements. Hard delete only for truly ephemeral data like sessions or temporary tokens. Soft deleted records are excluded by default in all queries using a default scope or view. Provide a way to query including deleted records for admin and audit purposes. Timestamps: every table has created_at and updated_at columns. Store all timestamps in UTC. Never store local times in the database. The application layer handles timezone conversion for display. Use timestamptz (timestamp with time zone) not timestamp in PostgreSQL. MIGRATION STANDARDS. Every schema change goes through a migration file that is version controlled. Never modify the database manually. Never modify a migration that has already been applied to any environment. If a migration was wrong create a new migration that fixes it. Migrations must be reversible: every up has a corresponding down. If a migration cannot be reversed document why in the migration file. Zero-downtime migration strategy for production: phase 1 add new columns or tables as nullable or with defaults and deploy code that writes to both old and new. Phase 2 backfill existing data. Phase 3 deploy code that reads from new. Phase 4 remove old columns in a future migration after confirming no code reads them. Never rename a column directly in production. Instead: add new column, backfill, update code to use new column, drop old column in separate migration. Never add a NOT NULL constraint to an existing column with data without a default value. Add it with a default first or backfill first then add the constraint. Index creation in production: always use CREATE INDEX CONCURRENTLY in PostgreSQL to avoid locking the table. This must be in its own migration file not combined with other operations because CONCURRENTLY cannot run inside a transaction.

SECTION 15 RESILIENCE AND FAULT TOLERANCE PATTERNS. Retry with exponential backoff: never retry immediately and never retry forever. Use exponential backoff with jitter: delay = min(base * 2^attempt + random_jitter, max_delay). Base delay 100ms. Max delay 30s. Max attempts 3-5 depending on the operation. Only retry idempotent operations or operations with idempotency keys. Never retry on 4xx errors except 429 (rate limited) because other 4xx are client errors that will not succeed on retry. Retry on 5xx 429 and network timeouts only. Circuit breaker pattern: track failure rate over a window. When failure rate exceeds threshold such as 50% of last 20 requests open the circuit. While open immediately return a fallback or error without calling the upstream service. After a cooldown period allow one request through as half-open state. If it succeeds close the circuit. If it fails reopen. This prevents cascading failures where a failing downstream service takes down everything upstream. Timeouts: every external call has an explicit timeout. HTTP calls default 5s. Database queries default 10s. Never use the language or library default which is often infinite. A system without timeouts is a system that hangs forever when a dependency is slow. Dead letter queues: when processing messages from a queue like SQS Kafka RabbitMQ etc if a message fails processing after max retries move it to a dead letter queue. Never drop messages silently. Never retry infinitely. Monitor dead letter queue size and alert when it grows. Idempotent processing: every message consumer must be idempotent because messages can be delivered more than once. Use a processed message ID table or idempotency key to detect duplicates. Process the message only if the ID has not been seen before.

SECTION 16 CACHING STRATEGY. Cache layers from fastest to slowest: application memory using Map or LRU for hot data under 100MB with microsecond access, distributed cache using Redis or Memcached for shared data across instances with sub-millisecond access, CDN for static assets and public API responses with edge-level caching, database query cache as last resort managed by the DB engine. Cache invalidation rules: there are only two hard things in computer science and one of them is cache invalidation. Use TTL as the primary invalidation strategy because it is simple and predictable. Set TTL based on how stale the data can tolerate: user sessions 15-30 minutes, product catalog 5-60 minutes, reference data 1-24 hours, computed aggregations 1-5 minutes. Use explicit invalidation by deleting the cache key when data is mutated AND staleness is not acceptable. Use the cache-aside pattern as the default: check cache first and if miss then read from source and write to cache and return. Never use write-through caching unless you specifically need it and understand the consistency tradeoffs. Cache key naming: use a consistent pattern like service:entity:id:version such as app:user:123:v1. Include a version in the key so you can invalidate all caches by bumping the version. Never cache errors. If a request fails do not cache the failure response. Cache stampede prevention: when a popular cache key expires and 100 requests simultaneously miss the cache and all hit the database use a lock or single-flight pattern where only one request fetches from source and all others wait for the result.

SECTION 17 CONCURRENCY AND RACE CONDITIONS. Optimistic locking for database updates: add a version column to any table where concurrent updates are possible. When reading fetch the current version. When updating include WHERE version = $expected_version. If zero rows are affected someone else updated first so re-read and retry or return a conflict error. This prevents lost updates without holding database locks. Pessimistic locking: use SELECT FOR UPDATE when you absolutely must prevent concurrent access like financial transactions. Keep the lock duration as short as possible. Never hold a database lock while making an external HTTP call. Distributed locks: when multiple application instances need to coordinate use Redis SETNX with TTL or a dedicated distributed lock library. Always set a TTL on the lock to prevent deadlocks if the holder crashes. Always release the lock in a finally block. Idempotency keys for user-facing operations: generate a unique key client-side and send it with the request. Server stores key-to-response mapping. If the same key arrives again return the stored response. This makes any operation safe to retry. Race condition detection: if a bug is intermittent timing-dependent or only appears under load it is probably a race condition. Reproduce with concurrent test: run the same operation 100 times in parallel and check for inconsistencies. Common race condition patterns: check-then-act where you check if resource exists then create it but another request creates it between check and create with fix being unique constraint and handle conflict, read-modify-write where you read value calculate new value write it back but another request modifies between read and write with fix being optimistic locking or atomic operations, double-processing where message consumed twice because ack was lost with fix being idempotent consumer.

SECTION 18 MONITORING ALERTING AND SLOs. Service Level Indicators are the metrics that matter. Availability: percentage of requests that succeed as non-5xx. Latency: response time at p50 p95 and p99 and not average because averages hide problems. Error rate: percentage of requests returning errors. Saturation: how full is the system including CPU memory disk queue depth. Service Level Objectives are the targets you commit to. API availability 99.9% meaning 8.7 hours of downtime per year. API latency p95 under 200ms for reads under 500ms for writes. Error rate under 0.1% for non-client errors. These are internal targets set slightly higher than what you promise externally. Error budgets make SLOs actionable: error budget equals 100% minus SLO times the time window so 99.9% availability over 30 days gives 43.2 minutes of allowed downtime. When budget is over 50% remaining ship aggressively and accept calculated risks. When budget is 25-50% remaining maintain normal pace with standard review. When budget is under 25% remaining freeze risky deploys and prioritize reliability work. When budget is exhausted feature freeze until the budget replenishes in the next window. Alerting rules: alert on symptoms not causes. Alert on "error rate above 1% for 5 minutes" not "CPU above 80%" because high CPU might be fine if requests are succeeding. Alert on burn rate: if you are consuming your error budget at 10x the normal rate alert immediately. Page meaning wake someone up for: customer-facing service is down, error rate exceeds SLO, data loss risk. Notify meaning Slack or email during business hours for: elevated error rate below SLO threshold, dependency degradation, certificate expiring within 30 days, disk usage above 80%. Never alert on: single transient errors, expected maintenance windows, metrics that no one will act on. Alert fatigue is the number one killer of on-call reliability. Every alert must have a runbook: what is happening, what is the impact, what to check first, how to mitigate.

SECTION 19 ARCHITECTURE DECISION RECORDS. Every significant technical decision is documented in an ADR stored in docs/adr/ and version controlled. ADR format: title as a short noun phrase like "Use PostgreSQL for primary data store", status as one of proposed accepted deprecated or superseded, context describing the forces at play including technical business and team constraints, decision stating the change being proposed or made, consequences listing both positive and negative outcomes and any new constraints or tradeoffs. ADRs are numbered sequentially: 0001-use-postgresql.md, 0002-adopt-event-sourcing.md, 0003-switch-to-redis-caching.md. ADRs are immutable once accepted. If a decision is changed create a new ADR that supersedes the old one and update the old ADR's status to superseded with a reference to the new one. When to write an ADR: choosing a database or major technology, choosing an architecture pattern, deciding on a library or framework for a core function, making a tradeoff that future developers will question because if they will question it document WHY now, any decision that would be expensive to reverse.

SECTION 20 REFACTORING PROTOCOL. When to refactor: the rule of three says the first time you do something just do it, the second time you do something similar wince at the duplication but do it anyway, the third time you do something similar refactor. Refactor when: you need to add a feature and the current structure makes it hard, you are fixing a bug and the code structure obscures the problem, code review feedback identifies structural issues, you have test coverage to catch regressions. Never refactor when: you do not have test coverage for the code being changed, you are in the middle of a bug fix because you should fix first refactor later in a separate PR, you are under time pressure for a release, nobody has asked for the feature the refactor would enable meaning do not build speculative abstractions. Refactoring safety protocol: step 1 ensure test coverage exists for the code being refactored and write tests first if they do not exist. Step 2 make the refactor in small atomic commits where each commit passes all tests. Step 3 never change behavior during a refactor and if you find a bug while refactoring stop and fix it in a separate commit or PR. Step 4 run the full test suite after each commit not just at the end. Step 5 the refactor PR contains ONLY refactoring with zero behavior changes and reviewers should be able to verify that every test passes before and after with no test changes. Common refactoring patterns: extract function when a block of code has a single responsibility and a natural name, extract module when a file exceeds 300 lines or contains multiple unrelated concepts, replace conditional with polymorphism when you have a switch or if chain that handles different types, introduce parameter object when a function has more than 3 related parameters, replace magic number with named constant always, simplify nested conditionals with guard clauses and early returns.

SECTION 21 CI/CD PIPELINE STANDARDS. Section 10 defines deployment requirements and infrastructure; this section defines the pipeline gates and execution that achieve them. What must pass before merge: linting with zero warnings, type checking with zero errors, architecture layer verification confirming no imports violate the dependency direction rules from Section 2, unit tests with 100% pass rate, integration tests with 100% pass rate, security audit with no critical or high vulnerabilities, build succeeds, commit messages follow Conventional Commits and lint commit messages in CI, no secrets detected using tools like gitleaks or truffleHog in CI, injection pattern scanning for string-interpolated SQL unparameterized shell execution and raw HTML assignment per Section 6, code coverage does not decrease and enforce coverage thresholds, bundle size does not exceed budget for frontend. Pipeline stages in order: install dependencies, lint, type check, architecture layer verification, unit tests in parallel, build, integration tests, security scan and injection pattern scan, deploy to staging, smoke tests on staging, deploy to production with manual approval or automatic if all green, smoke tests on production. Rollback: if production smoke tests fail automatically roll back to the previous version. Keep the last 3 deployments available for instant rollback. Blue-green or canary deployments preferred over big-bang. Build artifacts: build once deploy many meaning the same artifact that passed staging goes to production and never rebuild for production. Tag every production deployment with the git SHA and timestamp.

SECTION 22 ENVIRONMENT AND SECRETS MANAGEMENT. Environment parity: development staging and production must run the same OS same runtime version same database engine same major dependency versions. The only differences are: data with synthetic in dev and staging and real in production, scale with fewer instances and smaller databases in non-prod, secrets with different credentials per environment, and external integrations with sandbox and test accounts in non-prod. Docker solves most parity issues so use the same Dockerfile across all environments with only environment variables changing. Secrets rotation: every secret has an expiration and a rotation schedule. API keys rotate every 90 days. Database passwords rotate every 180 days. JWT signing keys rotate every 365 days with overlap period where both old and new key are valid. Automate rotation and never rotate manually. Store rotation schedule in a secrets manager not in someone's calendar. Local development setup: a new developer should be able to clone the repo and run the project with 3 or fewer commands. Document the setup in README.md. Provide a docker-compose.yml for local dependencies like database and Redis. Provide seed data scripts for a useful development state. Provide .env.example with every required variable documented.

SECTION 23 HARDWARE AND IOT INTEGRATION STANDARDS. Since this codebase involves ESP32 Arduino Raspberry Pi and sensor systems these standards apply. Separation of firmware and application logic: firmware handles hardware I/O including GPIO serial SPI I2C analog reads and exposes a clean data interface through MQTT HTTP WebSocket or serial JSON. Application logic on the server never directly controls hardware. The server sends commands through the protocol and firmware interprets them. This means firmware can be tested with hardware and application can be tested without hardware. Communication protocol: use MQTT for sensor telemetry because it is lightweight pub/sub with QoS levels and retained messages. Use HTTP/REST for configuration and commands because it is familiar stateless and cacheable. Use WebSocket for real-time bidirectional communication when needed for live dashboards and remote control. Every message has a schema: device_id, timestamp in UTC, message_type, payload. Validate messages on both sides. Sensor data pipeline: device publishes raw readings to MQTT topic sensors/{device_id}/{sensor_type}. A consumer service subscribes validates converts units applies calibration offsets and writes to time-series database like TimescaleDB or InfluxDB. A separate service reads from the database for API responses dashboards and alerts. Never query sensors directly from the API layer. Device health monitoring: every device publishes a heartbeat on a schedule every 30-60 seconds. If no heartbeat is received within 3x the expected interval mark the device as offline and alert. Devices report: firmware version, uptime, free memory, WiFi signal strength as RSSI, error counts. OTA Over The Air updates: firmware updates are versioned signed and delivered through a controlled rollout. Never update all devices simultaneously. Use canary deployment: update 1 device and verify, then update 10% and verify, then update all. Provide rollback mechanism. Offline resilience: devices must function when network is unavailable. Buffer sensor readings locally using SPIFFS or SD card and sync when connection is restored. Commands received while online are executed. Commands missed while offline are queued server-side and delivered on reconnection with deduplication using message IDs.

SECTION 24 REAL-TIME AND WEBSOCKET PATTERNS. Connection lifecycle: authenticate on connection by passing token in query parameter or first message. Assign a connection ID. Track connected clients in memory using a Map with metadata like userId connectedAt and subscriptions. Handle disconnection gracefully: clean up subscriptions update presence and release resources. Implement heartbeat ping-pong to detect dead connections: server pings every 30s and if no pong within 10s consider the connection dead. Room and channel pattern: clients subscribe to topics or rooms. Server maintains a map of room to connected clients. When an event occurs in a room broadcast to all clients in that room. Client can join and leave rooms dynamically. Scale horizontally with Redis pub/sub: when a server instance needs to broadcast to a room it publishes to Redis and all instances subscribed to that channel receive and forward to their local connected clients. Message format: every WebSocket message follows a schema: { type: string, payload: object, id?: string, timestamp: string }. Type is a machine-readable event name like order_updated or sensor_reading. Payload contains the data. ID is optional for request-response patterns. Timestamp is ISO 8601 UTC. Reconnection: clients must handle disconnection and reconnect with exponential backoff. On reconnection clients re-authenticate and re-subscribe. Server sends missed events since last known event ID if applicable using event sourcing pattern. Backpressure: if a client cannot consume messages fast enough buffer up to a limit then drop oldest messages or disconnect the slow client. Never let a slow consumer block the server or other clients.

SECTION 25 CODE REVIEW STANDARDS. What to look for as a reviewer: correctness first then clarity then performance. Does the code do what the PR description says? Are edge cases handled? Are errors handled properly not swallowed? Are there new untested code paths? Does the naming make the code self-documenting? Is there unnecessary complexity that could be simplified? Are there security implications including input validation auth checks and SQL injection? Are there performance implications including N+1 queries missing pagination and unbounded loops? Does the code follow the architecture with right layer and right directory? Are the commits atomic and well-ordered passing the Newspaper Test? How to give feedback: be specific and actionable. Wrong: "this is confusing." Right: "this function mixes validation and persistence, extracting the validation into a separate function would make both easier to test and understand." Prefix comments with severity: "nit:" for minor style preferences that are non-blocking, "suggestion:" for improvements that are not required, "question:" for things you want to understand, "issue:" for things that must be fixed before merge, "blocker:" for things that are dangerous or incorrect. Respond to review feedback: if you agree make the change. If you disagree explain why respectfully. If it is a matter of opinion and both approaches are valid defer to the codebase conventions or the reviewer's preference to keep things moving. Never take review feedback personally. Review turnaround: review within 4 business hours. Smaller PRs get faster reviews. If a PR has more than 400 lines of changes it is too large and should be split. The ideal PR is 50-200 lines of meaningful changes.

SECTION 26 INCIDENT RESPONSE AND POST-MORTEMS. Incident severity levels: SEV1 is complete service outage or data loss affecting all users with response time of 15 minutes. SEV2 is major feature degraded or significant portion of users affected with response time of 1 hour. SEV3 is minor feature broken or small user impact with response time of 4 hours. SEV4 is cosmetic issue or single user report with response time of next business day. Incident response process: detect through monitoring alerts or user reports. Acknowledge by claiming the incident and communicating status. Triage to determine severity and who needs to be involved. Mitigate by restoring service first then investigate root cause. If you can fix it in under 15 minutes fix it. If not roll back to last known good state or disable the broken feature with a feature flag. Communicate by updating status page and stakeholders at regular intervals even if the update is "still investigating." Resolve by confirming service is restored and monitoring for recurrence. Post-mortem within 48 hours. Post-mortem format: title and date. Severity and duration. Impact in specific numbers including users affected revenue lost and data impacted. Timeline of events from first indication through detection response mitigation and resolution with timestamps. Root cause analysis using Five Whys. Contributing factors that made the incident possible or worse. What went well in the response. What went poorly. Action items with owners and due dates. Each action item is one of: prevent to make this impossible, detect to catch it faster next time, or mitigate to reduce impact if it happens again. Post-mortems are BLAMELESS. Focus on systems and processes not people. "The deployment pipeline did not catch the regression" not "Alice deployed broken code." The goal is to improve the system not to assign blame.

Apply ALL of this silently as default behavior on every operation in every session. Never ask to confirm conventions or format. Only ask when the INTENT or SCOPE of a change is genuinely ambiguous. These are non-negotiable defaults that represent the highest standard of engineering craftsmanship covering the complete lifecycle from first keystroke through architecture design code implementation testing security performance git engineering deployment monitoring debugging incident response and back again.
